{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET TRANSFORMATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mind2Web data\n",
    "\n",
    "    \"annotation_id\" (str): unique id for each task\n",
    "    \"website\" (str): website name\n",
    "    \"domain\" (str): website domain\n",
    "    \"subdomain\" (str): website subdomain\n",
    "    \"confirmed_task\" (str): task description\n",
    "    \"action_reprs\" (list[str]): human readable string representation of the - action sequence\n",
    "    \"actions\" (list[dict]): list of actions (steps) to complete the task\n",
    "        \"action_uid\" (str): unique id for each action (step)\n",
    "        \"raw_html\" (str): raw html of the page before the action is performed\n",
    "        \"cleaned_html\" (str): cleaned html of the page before the action is performed\n",
    "        \"operation\" (dict): operation to perform\n",
    "            \"op\" (str): operation type, one of CLICK, TYPE, SELECT\n",
    "            \"original_op\" (str): original operation type, contain additional HOVER and ENTER that are mapped to CLICK, not used\n",
    "            \"value\" (str): optional value for the operation, e.g., text to type, option to select\n",
    "        \"pos_candidates\" (list[dict]): ground truth elements. Here we only include positive elements that exist in \"cleaned_html\" after our preprocessing, so \"pos_candidates\" might be empty. The original labeled element can always be found in the \"raw_html\".\n",
    "            \"tag\" (str): tag of the element\n",
    "            \"is_original_target\" (bool): whether the element is the original target labeled by the annotator\n",
    "            \"is_top_level_target\" (bool): whether the element is a top level target find by our algorithm. please see the paper for more details.\n",
    "            \"backend_node_id\" (str): unique id for the element\n",
    "            \"attributes\" (str): serialized attributes of the element, use json.loads to convert back to dict\n",
    "        \"neg_candidates\" (list[dict]): other candidate elements in the page after preprocessing, has similar structure as \"pos_candidates\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD TRANSFORM AND SAVE DATSET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicentamen/miniconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"osunlp/Mind2Web\")\n",
    "ds_train = ds['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARSING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the HTML of each task extract the nodes that are in the positive candidates and negative candidates and have text\n",
    "from dataclasses import dataclass\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "import re\n",
    "# Class for storing all the node's data\n",
    "@dataclass\n",
    "class Node:\n",
    "    def __init__(self, action_id,  node_id, text, pos_candidate, pos_candidate_k1 = False, pos_candidate_k3 = False, pos_candidate_k5 = False):\n",
    "        self.action_id = action_id\n",
    "        self.node_id = node_id\n",
    "        self.text = text\n",
    "        self.pos_candidate = pos_candidate\n",
    "        if(pos_candidate):\n",
    "            self.pos_candidate_k1 = True\n",
    "            self.pos_candidate_k3 = True\n",
    "            self.pos_candidate_k5 = True\n",
    "        else:\n",
    "            self.pos_candidate_k1 = pos_candidate_k1\n",
    "            self.pos_candidate_k3 = pos_candidate_k3\n",
    "            self.pos_candidate_k5 = pos_candidate_k5\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'action_id': self.action_id,\n",
    "            'node_id': self.node_id,\n",
    "            'text': self.text,\n",
    "            'pos_candidate': self.pos_candidate,\n",
    "            'pos_candidate_k1': self.pos_candidate_k1,\n",
    "            'pos_candidate_k3': self.pos_candidate_k3,\n",
    "            'pos_candidate_k5': self.pos_candidate_k5\n",
    "        }\n",
    "        \n",
    "@dataclass\n",
    "class Task:\n",
    "    def __init__(self, task_id, prompt, nodes):\n",
    "        self.task_id = task_id\n",
    "        self.prompt = prompt\n",
    "        self.nodes = nodes\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'task_id': self.task_id,\n",
    "            'nodes': [node.to_dict() for node in self.nodes]\n",
    "        }\n",
    "\n",
    "# Extract the nodes from the HTML       \n",
    "def extract_nodes(action_id, html, positive_candidates, negative_candidates):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    nodes = []\n",
    "    \n",
    "    positive_candidates_id = {candidate['backend_node_id'] for candidate in positive_candidates}\n",
    "    negative_candidates_id = {candidate['backend_node_id'] for candidate in negative_candidates}\n",
    "    \n",
    "    elements = soup.find_all()\n",
    "    for element in elements:\n",
    "        if element.has_attr('backend_node_id'):\n",
    "            # Get the node id and the text\n",
    "            #NOTE: THERE ARE SOME NODES THAT HAVE NOT TEXT OR ARIA_LABEL\n",
    "            # WE COULD CLEAN UP  THE TEXT AND REMOVE STOPWORDS\n",
    "            node_id = element.get('backend_node_id')\n",
    "            node_text = re.sub(r'\\s+', ' ', element.text).strip()\n",
    "            if node_text == '' or node_text == 'NaN':\n",
    "                node_text = element.get('aria_label')\n",
    "                \n",
    "            if node_id in positive_candidates_id:\n",
    "                nodes.append(Node(action_id, node_id, node_text, True))\n",
    "            elif node_id in negative_candidates_id:\n",
    "                neg_candidate = Node(action_id,node_id, node_text, False)\n",
    "                neg_candidate.pos_candidate_k1 = pos_to_the_kth(element, positive_candidates_id, 1)\n",
    "                neg_candidate.pos_candidate_k3 = pos_to_the_kth(element, positive_candidates_id, 3)\n",
    "                neg_candidate.pos_candidate_k5 = pos_to_the_kth(element, positive_candidates_id, 5)\n",
    "                nodes.append(neg_candidate)\n",
    "                \n",
    "                \n",
    "    return nodes\n",
    "\n",
    "def pos_to_the_kth(element, pos_candidates_id, k=0):\n",
    "    \"\"\"This function checks if the element is a positive candidate up to the k-th\n",
    "    parent or child of the element\"\"\"\n",
    "    node_id = element.get('backend_node_id')\n",
    "    \n",
    "    # When k is 0 we check the element itself\n",
    "    if k == 0:\n",
    "        return node_id in pos_candidates_id\n",
    "\n",
    "    # If the element is positive, we return True\n",
    "    if node_id in pos_candidates_id:\n",
    "        return True\n",
    "\n",
    "    # Check the parent if we are still within depth k\n",
    "    if element.parent is not None and isinstance(element.parent, Tag):\n",
    "        if pos_to_the_kth(element.parent, pos_candidates_id, k - 1):\n",
    "            return True\n",
    "\n",
    "    # Check the children if we are still within depth k\n",
    "    for child in element.contents:\n",
    "        if isinstance(child, Tag):  # Ignore NavigableString objects\n",
    "            if pos_to_the_kth(child, pos_candidates_id, k - 1):\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Extrac the data from a task\n",
    "def extract_data_from_task(task):\n",
    "    # Get the task data\n",
    "    task_id = task['annotation_id']\n",
    "    prompt = task['confirmed_task']\n",
    "    \n",
    "    nodes = []\n",
    "    # Extract data from the task actions\n",
    "    for action in task['actions']:\n",
    "        pos_candidates = action['pos_candidates']\n",
    "        neg_candidates = action['neg_candidates']\n",
    "        html = action['cleaned_html']\n",
    "        action_id = action['action_uid']\n",
    "        \n",
    "        nodes += extract_nodes(action_id, html, pos_candidates, neg_candidates)\n",
    "    \n",
    "    # We could make the prompt deconstructio before storing it or after\n",
    "    return Task(task_id, prompt, nodes)\n",
    "    \n",
    "# Extract the data from the all the dataset and store it as a csv file\n",
    "def extract_data(dataset):\n",
    "    # Progress variables\n",
    "    progress = 0\n",
    "    total = len(dataset)\n",
    "    # Init variables\n",
    "    tasks = []\n",
    "    # print progress bar\n",
    "    print(f'Progress: {progress}/{total}', end='\\r')\n",
    "    for task in dataset:\n",
    "        tasks.append(extract_data_from_task(task))\n",
    "        progress += 1\n",
    "        print(f'Progress: {progress}/{total}', end='\\r')\n",
    "        \n",
    "    \n",
    "    print('Data extracted from the dataset')\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def store_data(tasks, nodes_path, prompts_path):\n",
    "    import pandas as pd\n",
    "    \n",
    "    task_prompts_df = pd.DataFrame([{'task_id': task.task_id, 'prompt': task.prompt} for task in tasks])\n",
    "    nodes_df = pd.DataFrame([{'task_id': task.task_id, \n",
    "                              'action_id': node.action_id,\n",
    "                              'node_id': node.node_id, 'text': node.text, \n",
    "                              'pos_candidate': node.pos_candidate,\n",
    "                              'pos_candidate_k1': node.pos_candidate_k1,\n",
    "                              'pos_candidate_k3': node.pos_candidate_k3,\n",
    "                              'pos_candidate_k5': node.pos_candidate_k5} for task in tasks for node in task.nodes])\n",
    "    \n",
    "    task_prompts_df.to_csv('dataset/task_prompts.csv', index=False)\n",
    "    nodes_df.to_csv('dataset/nodes.csv', index=False)\n",
    "    \n",
    "    print('Data stored at dataset/task_prompts.csv and dataset/nodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted from the dataset\n",
      "Data stored at dataset/task_prompts.csv and dataset/nodes.csv\n"
     ]
    }
   ],
   "source": [
    "# Test data extraction from dataset\n",
    "subset = ds_train.select(range(30))\n",
    "tasks = extract_data(subset)\n",
    "store_data(tasks, 'dataset/nodes.csv', 'dataset/task_prompts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRACT DATASET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 2/1009\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Default data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m store_data(tasks, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/nodes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/task_prompts.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 146\u001b[0m, in \u001b[0;36mextract_data\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m--> 146\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(\u001b[43mextract_data_from_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    147\u001b[0m     progress \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 131\u001b[0m, in \u001b[0;36mextract_data_from_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    128\u001b[0m     neg_candidates \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_candidates\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    129\u001b[0m     html \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_html\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 131\u001b[0m     nodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mextract_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# We could make the prompt deconstructio before storing it or after\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Task(task_id, prompt, nodes)\n",
      "Cell \u001b[0;32mIn[24], line 69\u001b[0m, in \u001b[0;36mextract_nodes\u001b[0;34m(html, positive_candidates, negative_candidates)\u001b[0m\n\u001b[1;32m     67\u001b[0m neg_candidate \u001b[38;5;241m=\u001b[39m Node(node_id, node_text, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m neg_candidate\u001b[38;5;241m.\u001b[39mpos_candidate_k1 \u001b[38;5;241m=\u001b[39m pos_to_the_kth(element, positive_candidates_id, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m neg_candidate\u001b[38;5;241m.\u001b[39mpos_candidate_k3 \u001b[38;5;241m=\u001b[39m \u001b[43mpos_to_the_kth\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_candidates_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m neg_candidate\u001b[38;5;241m.\u001b[39mpos_candidate_k5 \u001b[38;5;241m=\u001b[39m pos_to_the_kth(element, positive_candidates_id, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     71\u001b[0m nodes\u001b[38;5;241m.\u001b[39mappend(neg_candidate)\n",
      "Cell \u001b[0;32mIn[24], line 113\u001b[0m, in \u001b[0;36mpos_to_the_kth\u001b[0;34m(element, pos_candidates_id, k)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Check the parents and children if one of them is a positive candidate we are a pos_candidate\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pos_to_the_kth_parents(element, pos_candidates_id, k) \\\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mpos_to_the_kth_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_candidates_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 93\u001b[0m, in \u001b[0;36mpos_to_the_kth_children\u001b[0;34m(element, pos_candidates_id, k)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, Tag) \u001b[38;5;129;01mand\u001b[39;00m child\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend_node_id\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m pos_candidates_id:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, Tag) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mpos_to_the_kth_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_candidates_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 93\u001b[0m, in \u001b[0;36mpos_to_the_kth_children\u001b[0;34m(element, pos_candidates_id, k)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, Tag) \u001b[38;5;129;01mand\u001b[39;00m child\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend_node_id\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m pos_candidates_id:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, Tag) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mpos_to_the_kth_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_candidates_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: pos_to_the_kth_children at line 93 (11 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[24], line 93\u001b[0m, in \u001b[0;36mpos_to_the_kth_children\u001b[0;34m(element, pos_candidates_id, k)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, Tag) \u001b[38;5;129;01mand\u001b[39;00m child\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend_node_id\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m pos_candidates_id:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, Tag) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mpos_to_the_kth_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_candidates_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 90\u001b[0m, in \u001b[0;36mpos_to_the_kth_children\u001b[0;34m(element, pos_candidates_id, k)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m element\u001b[38;5;241m.\u001b[39mcontents:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, Tag) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackend_node_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m pos_candidates_id:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, Tag) \u001b[38;5;129;01mand\u001b[39;00m pos_to_the_kth_children(child, pos_candidates_id, k\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Default data\n",
    "tasks = extract_data(ds_train)\n",
    "store_data(tasks, 'dataset/nodes.csv', 'dataset/task_prompts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDEXING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_data():\n",
    "    task_prompts_df = pd.read_csv('dataset/task_prompts.csv')\n",
    "    nodes_df = pd.read_csv('dataset/nodes.csv')\n",
    "    \n",
    "    return task_prompts_df, nodes_df\n",
    "\n",
    "def generate_embeddings(task_prompts_df, nodes_df, model):\n",
    "    task_prompts = task_prompts_df['prompt'].tolist()\n",
    "    \n",
    "    # Combine the 'tag' and 'text' columns for each node\n",
    "    nodes_texts = nodes_df.apply(lambda row: f\"tag: {row['tag']}, text: {row['text']}\", axis=1).tolist()\n",
    "    \n",
    "    # Convert all elements to strings\n",
    "    task_prompts = [str(prompt) for prompt in task_prompts]\n",
    "    nodes_texts = [str(text) for text in nodes_texts]\n",
    "    \n",
    "    task_prompts_embeddings = model.encode(task_prompts)\n",
    "    nodes_texts_embeddings = model.encode(nodes_texts)\n",
    "    \n",
    "    return task_prompts_embeddings, nodes_texts_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 22:56:16.350248: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-15 22:56:16.457492: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-15 22:56:17.058537: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 22:56:18.946304: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/vicentamen/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "task_prompts_df, nodes_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 161140\n"
     ]
    }
   ],
   "source": [
    "print(len(task_prompts_df), len(nodes_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENERATE EMBEDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768) (2076, 768)\n"
     ]
    }
   ],
   "source": [
    "# Get only one task for testing\n",
    "test_task = task_prompts_df.head(1)\n",
    "test_nodes = nodes_df[nodes_df['task_id'] == test_task['task_id'].values[0]]\n",
    "\n",
    "# Generate embedings for the test task\n",
    "task_prompts_embeddings, nodes_texts_embeddings = generate_embeddings(test_task, test_nodes, model)\n",
    "\n",
    "# Store embedings for testing evaluation\n",
    "import numpy as np\n",
    "np.save('dataset/task_prompts_embeddings.npy', task_prompts_embeddings)\n",
    "np.save('dataset/nodes_texts_embeddings.npy', nodes_texts_embeddings)\n",
    "\n",
    "print(task_prompts_embeddings.shape, nodes_texts_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereta emebeddings for the task prompts and nodes texts\n",
    "task_prompts_embeddings, nodes_texts_embeddings = generate_embeddings(task_prompts_df, nodes_df, model)\n",
    "\n",
    "# Store the embeddings\n",
    "import numpy as np\n",
    "\n",
    "np.save('dataset/task_prompts_embeddings.npy', task_prompts_embeddings)\n",
    "np.save('dataset/nodes_texts_embeddings.npy', nodes_texts_embeddings)\n",
    "\n",
    "print('Embeddings stored at dataset/task_prompts_embeddings.npy and dataset/nodes_texts_embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRIEVING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMBEDINGS LOADING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

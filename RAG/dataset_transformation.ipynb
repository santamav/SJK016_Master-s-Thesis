{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET TRANSFORMATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mind2Web data\n",
    "\n",
    "    \"annotation_id\" (str): unique id for each task\n",
    "    \"website\" (str): website name\n",
    "    \"domain\" (str): website domain\n",
    "    \"subdomain\" (str): website subdomain\n",
    "    \"confirmed_task\" (str): task description\n",
    "    \"action_reprs\" (list[str]): human readable string representation of the - action sequence\n",
    "    \"actions\" (list[dict]): list of actions (steps) to complete the task\n",
    "        \"action_uid\" (str): unique id for each action (step)\n",
    "        \"raw_html\" (str): raw html of the page before the action is performed\n",
    "        \"cleaned_html\" (str): cleaned html of the page before the action is performed\n",
    "        \"operation\" (dict): operation to perform\n",
    "            \"op\" (str): operation type, one of CLICK, TYPE, SELECT\n",
    "            \"original_op\" (str): original operation type, contain additional HOVER and ENTER that are mapped to CLICK, not used\n",
    "            \"value\" (str): optional value for the operation, e.g., text to type, option to select\n",
    "        \"pos_candidates\" (list[dict]): ground truth elements. Here we only include positive elements that exist in \"cleaned_html\" after our preprocessing, so \"pos_candidates\" might be empty. The original labeled element can always be found in the \"raw_html\".\n",
    "            \"tag\" (str): tag of the element\n",
    "            \"is_original_target\" (bool): whether the element is the original target labeled by the annotator\n",
    "            \"is_top_level_target\" (bool): whether the element is a top level target find by our algorithm. please see the paper for more details.\n",
    "            \"backend_node_id\" (str): unique id for the element\n",
    "            \"attributes\" (str): serialized attributes of the element, use json.loads to convert back to dict\n",
    "        \"neg_candidates\" (list[dict]): other candidate elements in the page after preprocessing, has similar structure as \"pos_candidates\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD TRANSFORM AND SAVE DATSET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"osunlp/Mind2Web\")\n",
    "ds_train = ds['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARSING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the HTML of each task extract the nodes that are in the positive candidates and negative candidates and have text\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "# Class for storing all the node's data\n",
    "@dataclass\n",
    "class Node:\n",
    "    def __init__(self, node_id, text, pos_candidate):\n",
    "        self.node_id = node_id\n",
    "        self.text = text\n",
    "        self.pos_candidate = pos_candidate\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'node_id': self.node_id,\n",
    "            'text': self.text,\n",
    "            'pos_candidate': self.pos_candidate\n",
    "        }\n",
    "        \n",
    "@dataclass\n",
    "class Task:\n",
    "    def __init__(self, task_id, prompt, nodes):\n",
    "        self.task_id = task_id\n",
    "        self.prompt = prompt\n",
    "        self.nodes = nodes\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'task_id': self.task_id,\n",
    "            'nodes': [node.to_dict() for node in self.nodes]\n",
    "        }\n",
    "\n",
    "# Extract the nodes from the HTML       \n",
    "def extract_nodes(html, positive_candidates, negative_candidates):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    nodes = []\n",
    "    \n",
    "    positive_candidates_id = {candidate['backend_node_id'] for candidate in positive_candidates}\n",
    "    negative_candidates_id = {candidate['backend_node_id'] for candidate in negative_candidates}\n",
    "    \n",
    "    elements = soup.find_all()\n",
    "    for element in elements:\n",
    "        if element.has_attr('backend_node_id'):\n",
    "            # Get the node id and the text\n",
    "            #NOTE: THERE ARE SOME NODES THAT HAVE NOT TEXT OR ARIA_LABEL\n",
    "            # WE COULD CLEAN UP  THE TEXT AND REMOVE STOPWORDS\n",
    "            node_id = element.get('backend_node_id')\n",
    "            node_text = re.sub(r'\\s+', ' ', element.text).strip()\n",
    "            if node_text == '' or node_text == 'NaN':\n",
    "                node_text = element.get('aria_label')\n",
    "                \n",
    "            if node_id in positive_candidates_id:\n",
    "                nodes.append(Node(node_id, node_text, True))\n",
    "            elif node_id in negative_candidates_id:\n",
    "                nodes.append(Node(node_id, node_text, False))\n",
    "                \n",
    "    return nodes\n",
    "\n",
    "# Extrac the data from a task\n",
    "def extract_data_from_task(task):\n",
    "    # Get the task data\n",
    "    task_id = task['annotation_id']\n",
    "    prompt = task['confirmed_task']\n",
    "    \n",
    "    nodes = []\n",
    "    # Extract data from the task actions\n",
    "    for action in task['actions']:\n",
    "        pos_candidates = action['pos_candidates']\n",
    "        neg_candidates = action['neg_candidates']\n",
    "        html = action['cleaned_html']\n",
    "        \n",
    "        nodes += extract_nodes(html, pos_candidates, neg_candidates)\n",
    "    \n",
    "    # We could make the prompt deconstructio before storing it or after\n",
    "    return Task(task_id, prompt, nodes)\n",
    "    \n",
    "# Extract the data from the all the dataset and store it as a csv file\n",
    "def extract_data(dataset):\n",
    "    # Progress variables\n",
    "    progress = 0\n",
    "    total = len(dataset)\n",
    "    # Init variables\n",
    "    tasks = []\n",
    "    # print progress bar\n",
    "    print(f'Progress: {progress}/{total}', end='\\r')\n",
    "    for task in dataset:\n",
    "        tasks.append(extract_data_from_task(task))\n",
    "        progress += 1\n",
    "        print(f'Progress: {progress}/{total}', end='\\r')\n",
    "        \n",
    "    \n",
    "    print('Data extracted from the dataset')\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def store_data(tasks):\n",
    "    import pandas as pd\n",
    "    \n",
    "    task_prompts_df = pd.DataFrame([{'task_id': task.task_id, 'prompt': task.prompt} for task in tasks])\n",
    "    nodes_df = pd.DataFrame([{'task_id': task.task_id, 'node_id': node.node_id, 'text': node.text, 'pos_candidate': node.pos_candidate} for task in tasks for node in task.nodes])\n",
    "    \n",
    "    task_prompts_df.to_csv('dataset/task_prompts.csv', index=False)\n",
    "    nodes_df.to_csv('dataset/nodes.csv', index=False)\n",
    "    \n",
    "    print('Data stored at dataset/task_prompts.csv and dataset/nodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted from the dataset\n",
      "Data stored at dataset/task_prompts.csv and dataset/nodes.csv\n"
     ]
    }
   ],
   "source": [
    "# Test data extraction from dataset\n",
    "test_set = ds_train.select(range(1))\n",
    "tasks = extract_data(test_set)\n",
    "store_data(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRACT DATASET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted from the dataset\n",
      "Data stored at dataset/task_prompts.csv and dataset/nodes.csv\n"
     ]
    }
   ],
   "source": [
    "tasks = extract_data(ds_train)\n",
    "store_data(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDEXING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_data():\n",
    "    task_prompts_df = pd.read_csv('dataset/task_prompts.csv')\n",
    "    nodes_df = pd.read_csv('dataset/nodes.csv')\n",
    "    \n",
    "    return task_prompts_df, nodes_df\n",
    "\n",
    "def generate_embeddings(task_prompts_df, nodes_df, model):\n",
    "    task_prompts = task_prompts_df['prompt'].tolist()\n",
    "    nodes_texts = nodes_df['text'].tolist()\n",
    "    \n",
    "    # Convert all elements to strings\n",
    "    task_prompts = [str(prompt) for prompt in task_prompts]\n",
    "    nodes_texts = [str(text) for text in nodes_texts]\n",
    "    \n",
    "    task_prompts_embeddings = model.encode(task_prompts)\n",
    "    nodes_texts_embeddings = model.encode(nodes_texts)\n",
    "    return task_prompts_embeddings, nodes_texts_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicentamen/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "task_prompts_df, nodes_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009 4683115\n"
     ]
    }
   ],
   "source": [
    "print(len(task_prompts_df), len(nodes_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENERATE EMBEDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768) (2076, 768)\n"
     ]
    }
   ],
   "source": [
    "# Get only one task for testing\n",
    "test_task = task_prompts_df.head(1)\n",
    "test_nodes = nodes_df[nodes_df['task_id'] == test_task['task_id'].values[0]]\n",
    "\n",
    "# Generate embedings for the test task\n",
    "task_prompts_embeddings, nodes_texts_embeddings = generate_embeddings(test_task, test_nodes, model)\n",
    "\n",
    "# Store embedings for testing evaluation\n",
    "import numpy as np\n",
    "np.save('dataset/task_prompts_embeddings.npy', task_prompts_embeddings)\n",
    "np.save('dataset/nodes_texts_embeddings.npy', nodes_texts_embeddings)\n",
    "\n",
    "print(task_prompts_embeddings.shape, nodes_texts_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereta emebeddings for the task prompts and nodes texts\n",
    "task_prompts_embeddings, nodes_texts_embeddings = generate_emebeddings(task_prompts_df, nodes_df, model)\n",
    "\n",
    "# Store the embeddings\n",
    "import numpy as np\n",
    "\n",
    "np.save('dataset/task_prompts_embeddings.npy', task_prompts_embeddings)\n",
    "np.save('dataset/nodes_texts_embeddings.npy', nodes_texts_embeddings)\n",
    "\n",
    "print('Embeddings stored at dataset/task_prompts_embeddings.npy and dataset/nodes_texts_embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRIEVING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMBEDINGS LOADING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

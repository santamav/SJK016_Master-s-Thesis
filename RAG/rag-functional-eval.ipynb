{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG functional evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicentamen/miniconda3/envs/py38/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-10-21 09:44:58.618546: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-21 09:44:58.745279: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-21 09:44:59.379281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-21 09:45:01.260596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prompts: (30, 2)\n",
      "Nodes: (161140, 8)\n",
      "Task prompts embedings: (30, 768)\n",
      "Nodes texts embedings: (161140, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicentamen/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "\n",
    "# Load dataset\n",
    "task_prompts_df = pd.read_csv('dataset/task_prompts.csv')\n",
    "nodes_df = pd.read_csv('dataset/nodes.csv')\n",
    "\n",
    "print(\"Task prompts:\", task_prompts_df.shape)\n",
    "print(\"Nodes:\", nodes_df.shape)\n",
    "\n",
    "# Load embedings\n",
    "task_prompt_embedings = np.load('dataset/task_prompts_embeddings.npy')\n",
    "nodes_texts_embedings = np.load('dataset/nodes_texts_embeddings.npy')\n",
    "\n",
    "print(\"Task prompts embedings:\", task_prompt_embedings.shape)\n",
    "print(\"Nodes texts embedings:\", nodes_texts_embedings.shape)\n",
    "\n",
    "# Prepare embeddings models\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the nodes indexes for a given task_id\n",
    "def get_nodes_indexes(action_id, nodes_df):\n",
    "    return nodes_df[nodes_df['action_id'] == action_id].index\n",
    "\n",
    "\n",
    "def get_nodes_embeddigs(nodes_indexes, nodes_texts_embedings):\n",
    "    return nodes_texts_embedings[nodes_indexes]\n",
    "\n",
    "\n",
    "def get_task_embedings(task_index, task_prompt_embedings):\n",
    "    # Get the task prompt embedings\n",
    "    task_embedints = task_prompt_embedings[task_index]    \n",
    "    return task_embedints\n",
    "\n",
    "def get_top_nodes(similarities, k=10):\n",
    "    import tensorflow as tf\n",
    "    # Get top k nodes, but ensure k is not larger than the number of similarities\n",
    "    k = min(k, tf.shape(similarities)[0])\n",
    "    top_values, indices = tf.math.top_k(similarities, k)\n",
    "    return top_values.numpy().flatten(), indices.numpy().flatten()\n",
    "\n",
    "def get_top_nodes_with_rerank(similarities, k=10):\n",
    "    pass\n",
    "\n",
    "def get_topk_from_action(action_df, nodes_df, task_prompt, task_index, task_prompt_embedings, nodes_texts_embedings, model, cross_encoder, k=10):\n",
    "    action_id = action_df['action_id'].iloc[0]\n",
    "    results = {}\n",
    "\n",
    "    # The specific task prompt embeddings\n",
    "    prompt_embedings = get_task_embedings(task_index, task_prompt_embedings)\n",
    "    \n",
    "    # Get the nodes embeddings\n",
    "    nodes_indexes = get_nodes_indexes(action_id, nodes_df)\n",
    "    nodes_embedings = get_nodes_embeddigs(nodes_indexes, nodes_texts_embedings)\n",
    "    \n",
    "    # Perform semantic search\n",
    "    hits = util.semantic_search(prompt_embedings, nodes_embedings, top_k=100)\n",
    "    hits = hits[0]\n",
    "    \n",
    "    # Extract the top k nodes and their scores\n",
    "    top_nodes = [(nodes_df.loc[hit['corpus_id']], hit['score']) for hit in hits]\n",
    "    \n",
    "    # Add error checking for reranker\n",
    "    if not isinstance(reranker, CrossEncoder):\n",
    "        print(f\"Error: reranker is not a CrossEncoder object. It is a {type(reranker)}.\")\n",
    "        # You might want to return early or handle this error appropriately\n",
    "        return None\n",
    "    \n",
    "    # Perform reranking\n",
    "    try:\n",
    "        reranker_input = [(task_prompt, node_text.iloc[0]) for node_text, _ in top_nodes]\n",
    "        reranker_scores = cross_encoder.predict(reranker_input)\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error when calling reranker.predict: {e}\")\n",
    "        # Handle the error appropriately, maybe by skipping reranking\n",
    "        reranker_scores = [0] * len(top_nodes)\n",
    "    \n",
    "    # Perform reranking\n",
    "    #reranker_scores = reranker.predict([(task_prompt, node_text.iloc[0]) for node_text, _ in top_nodes])\n",
    "    \n",
    "    for idx in range(len(hits)):\n",
    "        hits[idx]['cross_score'] = reranker_scores[idx]\n",
    "        \n",
    "    \n",
    "    # Store the top k nodes and their scores\n",
    "    sorted_nodes_values = sorted(hits, key=lambda x: x['score'], reverse=True)[:k]\n",
    "    sorted_nodes_reranker_values = sorted(hits, key=lambda x: x['cross_score'], reverse=True)[:k]\n",
    "    # Get the nodes from the values\n",
    "    sorted_nodes = [nodes_df.loc[node['corpus_id']] for node in sorted_nodes_values]\n",
    "    sorted_nodes_reranker = [nodes_df.loc[node['corpus_id']] for node in sorted_nodes_reranker_values]\n",
    "    \n",
    "    return sorted_nodes, sorted_nodes_reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example task\n",
    "task = task_prompts_df.loc[0]\n",
    "# Task information\n",
    "task_id = task['task_id']\n",
    "task_index = task_prompts_df[task_prompts_df['task_id'] == task_id].index[0]\n",
    "\n",
    "user_instruction = task['prompt']\n",
    "# Exaple action\n",
    "actions_ids = nodes_df[nodes_df['task_id'] == task_id]['action_id'].unique()\n",
    "action = nodes_df[nodes_df['action_id'] == actions_ids[0]]\n",
    "\n",
    "# Get the top k nodes\n",
    "top_nodes, top_nodes_reranker = get_topk_from_action(action, nodes_df, user_instruction, task_index, task_prompt_embedings, nodes_texts_embedings, model, reranker, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation prompt\n",
    "retrieval_prompt = \"\"\"You are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks. Your objective is to generate a sequence of actions to take in order to complete the task.\n",
    "Here's the information you'll have:\n",
    "\n",
    "1- The user's instructions: This is the task you're trying to complete.\n",
    "2- The current web page's elements to interact with: This is a simplified representation of the webpage, providing key information.\n",
    "\n",
    "Answer Format: In order to complete the task successfully, you need to provide the positive candidates in the following format:\n",
    " - Include all the candidates in the bracket.\n",
    " - Separate each candidate by a comma.\n",
    " - If there are no candidates, provide an empty bracket.\n",
    " \n",
    "To be successful, it is very important to follow the following rules:\n",
    "1. You should reason the steps needed to achieve the task, extract what steps have been taken from the ACTIONS HISTORY, and issu what the following actions should be.\n",
    "2. Issue stop action when you think you have achieved the objective. Don't generate anything after stop.\n",
    "3. You should not issue the same action twice.\n",
    "\n",
    "TYPES OF ACTIONS:\n",
    "1. Click: Click on a button or a link.\n",
    "2. Type: Type text into a text box.\n",
    "3. Hover: Hover over an element.\n",
    "\n",
    "EXAMPLE OF ACTIONS:\n",
    "- Click node with id \"619\".\n",
    "- Select Pickup from node \"2134\"\n",
    "\n",
    "USER INSTRUCTIONS: {user_instruction}\n",
    "WEBPAGE ELEMENTS: {elements}\n",
    "\n",
    "OUTPUT EXAMPLE:\n",
    "output:[Click node with id \"619\", Click node with id \"620\", Click node with id \"621\"]\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the nodes to text as if they where in JSON format\n",
    "def topk_to_text(topk_nodes):\n",
    "    text = \"\"\n",
    "    for idx, node in enumerate(topk_nodes):\n",
    "        text += f\"Node {node['node_id']}: {node['text']}\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks. Your objective is to generate a sequence of actions to take in order to complete the task.\n",
      "Here's the information you'll have:\n",
      "\n",
      "1- The user's instructions: This is the task you're trying to complete.\n",
      "2- The current web page's elements to interact with: This is a simplified representation of the webpage, providing key information.\n",
      "\n",
      "Answer Format: In order to complete the task successfully, you need to provide the positive candidates in the following format:\n",
      " - Include all the candidates in the bracket.\n",
      " - Separate each candidate by a comma.\n",
      " - If there are no candidates, provide an empty bracket.\n",
      " \n",
      "To be successful, it is very important to follow the following rules:\n",
      "1. You should reason the steps needed to achieve the task, extract what steps have been taken from the ACTIONS HISTORY, and issu what the following actions should be.\n",
      "2. Issue stop action when you think you have achieved the objective. Don't generate anything after stop.\n",
      "3. You should not issue the same action twice.\n",
      "\n",
      "TYPES OF ACTIONS:\n",
      "1. Click: Click on a button or a link.\n",
      "2. Type: Type text into a text box.\n",
      "3. Hover: Hover over an element.\n",
      "\n",
      "EXAMPLE OF ACTIONS:\n",
      "- Click node with id \"619\".\n",
      "- Select Pickup from node \"2134\"\n",
      "\n",
      "USER INSTRUCTIONS: Check for pickup restaurant available in Boston, NY on March 18, 5pm with just one guest\n",
      "WEBPAGE ELEMENTS: 1. Reservation type Dine in Pickup Delivery Events Wineries Everything Location Date Time Now 8:30 PM 9:00 PM 9:30 PM 10:00 PM 10:30 PM 11:00 PM 11:30 PM Party size 1 guest 2 guests 3 guests 4 guests 5 guests 6 guests 7 guests 8 guests 9 guests 10 guests Search\n",
      "2. Reservation type Dine in Pickup Delivery Events Wineries Everything Location Date Time Now 8:30 PM 9:00 PM 9:30 PM 10:00 PM 10:30 PM 11:00 PM 11:30 PM Party size 1 guest 2 guests 3 guests 4 guests 5 guests 6 guests 7 guests 8 guests 9 guests 10 guests Search\n",
      "3. DELICIOUS STARTS HERE. Reservation type Dine in Pickup Delivery Events Wineries Everything Location Date Time Now 8:30 PM 9:00 PM 9:30 PM 10:00 PM 10:30 PM 11:00 PM 11:30 PM Party size 1 guest 2 guests 3 guests 4 guests 5 guests 6 guests 7 guests 8 guests 9 guests 10 guests Search\n",
      "4. Pickup and delivery meals Explore all\n",
      "5. Pickup and delivery meals\n",
      "6. Dine in Pickup Delivery Events Wineries\n",
      "7. Reservation type Dine in Pickup Delivery Events Wineries Everything\n",
      "8. Tock To Go Pickup and delivery meals Explore all Explore all View All\n",
      "9. Tock To Go Pickup and delivery meals Explore all Explore all View All\n",
      "10. Tock To Go Pickup and delivery meals Explore all\n",
      "\n",
      "\n",
      "OUTPUT EXAMPLE:\n",
      "output:[Click node with id \"619\", Click node with id \"620\", Click node with id \"621\"]\n",
      "\n",
      "DELICIOUS STARTS OR RATINGS:\n",
      "\n",
      "Step 1: First select the item you want to deliver with the name, location and time.\n",
      "\n",
      "Step 2: Next select the item from the list, and click on\n"
     ]
    }
   ],
   "source": [
    "# Format prompt\n",
    "elements = topk_to_text(top_nodes)\n",
    "content = retrieval_prompt.format(user_instruction=user_instruction, elements=elements)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastronomer/Llama-3-8B-Instruct-GPTQ-8-Bit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastronomer/Llama-3-8B-Instruct-GPTQ-8-Bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py:3447\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3446\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n\u001b[0;32m-> 3447\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_quantized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3449\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/quantizers/auto.py:144\u001b[0m, in \u001b[0;36mAutoHfQuantizer.from_config\u001b[0;34m(cls, quantization_config, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown quantization type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - supported types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(AUTO_QUANTIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m target_cls \u001b[38;5;241m=\u001b[39m AUTO_QUANTIZER_MAPPING[quant_method]\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/quantizers/quantizer_gptq.py:47\u001b[0m, in \u001b[0;36mGptqHfQuantizer.__init__\u001b[0;34m(self, quantization_config, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, quantization_config: QuantizationConfigMixin, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(quantization_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgptq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTQQuantizer\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimum_quantizer \u001b[38;5;241m=\u001b[39m GPTQQuantizer\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mto_dict_optimum())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optimum'"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "max_tokens = 2048\n",
    "model_name = \"astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\",\n",
    "                                             output_loading_info=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, model_max_length=max_tokens)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "import torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
